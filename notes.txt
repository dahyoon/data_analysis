데이터분석 자격증 시험:
- https://www.dataq.or.kr/www/sub/a_06.do
- ADP (Advanced Data Professional)
	- 전문가 - 실기 있음
- ADsP (Advanced Data Analytics Semi-Professional)
	- 준전문가 - 실기 없음
	- 코드: R
- 빅데이터분석기사 - 비 추천!!! (너무 어려움 - 수학공식)
	- 현업에서는 인정해 줌

내 컴퓨터에 설치되어있는 파이썬 패키지 목록 조회:
$ pip list
$ pip3 list

데이터 분석 도구:
- R (무료)
- Python (무료)
- SPSS (유료)
- QGIS - 지리정보 분석 (예, 입지 선정)
- GeoDA - 공간회귀 분석 (지역간 연관성)

1. 명령프롬프트 실행
	- WinKey + R -> cmd(엔터)
2. jupyter 설치
	- pip install --upgrade Jupyter      # --upgrade의 의미: 프로그램이 이미 깔려있으면 깔려있는 버전이 최신이 아닐 시 기존의 프로그램을 지우고 최신 버전으로 다운로드
3. jupyter 실행
	- jupyter lab   # 작업 폴더 위치에서 명령프롬프트 

접속 정보 (브라우저 및 os 버전 정보):
Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36

vscode setting
- 화면 확대/축소: command + '-' or '+'
- 색상 및 아이콘 테마: extensions에 가서 theme 골라서 적용
- 설정화면 열기: font size

6/19 월 5_카카오책검색:
- 내 애플리케이션 > treehouse(내 앱) > 앱 키 > REST API 키: f903e6bb99d409d576298b6e08527c66
- 문서 > Daum검색 > REST API > 웹문서 검색하기 > GET 의 URL: https://dapi.kakao.com/v2/search/web
- 요청 > 헤더 > Authorization > Authorization: KakaoAK ${REST_API_KEY}
- 인증 방식, 서비스 앱에서 REST API 키로 인증 요청
- 카카오는 "Referer": "" 을 줄 필요 없다; "User-Agent" (웹브라우저 버전 정보)는 옵션
- Meta란 본캐를 설명하는 부가정보
- DataFrame 클래스: 딕셔너리를 포함하는 리스트를 표 형태로 만들어줄 때 씀; 엑셀 시트 같이 생긴 자료구조


6/20 Crawling, HTML, CSS:
- scrapping: 단일 페이지로부터의 데이터 수집
- crawling: 특정 페이지로부터 파생되는 하위 페이지까지 일괄 수집
- crawler: 크롤링을 수행하는 소프트웨어 (=검색엔진)
- html 기본 구성 단축키: html:5 enter
- html은 골격 생성, css는 디자인 구성, javascript는 동작 부여
- markdown: (https://thisblogbusy.tistory.com/entry/%EB%A7%88%ED%81%AC%EB%8B%A4%EC%9A%B4Markdown-%EC%9D%B4%EB%9E%80)
	- html이 간결화된 글쓰기 도구
	- notion.so: markdown을 간편하게 쓸 수 있도록 만들어진 서비스/웹사이트
	- 태그 종류:
		- 제목 : # =====
		- 인용 : >
		- 강조 : * _ ** __ *** ___
		- 링크 : [텍스트](주소 "설명 생략가능")
		- 이미지 : ![텍스트](이미지주소 "설명 생략가능")
		- 리스트 : 1 * - +
		- 코드표시 : <code>코드</code> , 한줄 띄우고 스페이스 4칸 , ```코드```
		- 줄바꿈 : 엔터 2번 , 강제 줄바꿈은 문장끝에 스페이스바 2칸
		- 가로선 : ----- , ***** , +++++
- 안씨는 영어만 되지만, UTF-8에서는 한글명으로 변수 정의할 수 있음

6/21 Crawling, HTML, CSS:
- url = "https://www.coupang.com/np/search?q=%EB%85%B8%ED%8A%B8%EB%B6%81&listSize=72&channel=user"
	- 통신 방식의 프로토콜 (규약, 규격) - http (일반 웹, 보안x), https (보안 웹)
	- 실제 주소: www.coupang.com/np/search
		- 도메인(컴퓨터 주소): www.coupang.com
		- 파일 경로: /np/search
	- url encoding: q=%EB%85%B8%ED%8A%B8%EB%B6%81&listSize=72&channel=user 
		- ? 뒤에 변수이름=값&변수이름=값
		- 원본 --> 인코딩 --> 형식변환
		- 원본 <-- 디코딩 <-- 변환된 형식
- IP주소: 모든 컴퓨터에 부여되는 인터넷 상의 주소
	- IPv4방식인 숫자 4개로 이루어져있다: xxx.xxx.xxx.xxx 
	- 각 자리는 0-255의 자릿수로 이루어짐
	- 첫 3 자리는 네트워크 상의 그룹주소 (아파트의 동)
	- 마지막 자리는 컴퓨터주소 (아파트의 호)

6/26 Crawling:
- 다운되어있는 pip packages 조회: $ pip list
- selenium:
	- 파이썬, 자바스크립트, 자바, C++ 제공
	- 제어 -> chrome-driver.exe 제어-> chrome browser
	1) 특정 페이지 접속
	2) element 코드 가져오기
	3) 특정 요소(input)에 키보드 입력 보내기
	4) 마우스 클릭 보내기
	5) JS 코드 강제시행 => 스크롤 발생

6/27 SQL:
- 컴퓨터 구조
	1) 대상 컴퓨터의 IP주소를 알면 원격 접속 가능
	2) windows에 docker을 설치한 경우: window에 접속한 port를 docker의 port로 포워딩함
	3) windows에서 해당 port를 열면 (방화벽 해제) 외부에서 나의 docker안에 접속 가능
	4) IP확인

7/3 데이터 시각화:
1. 그래프
	1) 선 - 1갸의 변수, 시간의 흐름 (시계열 그래프)
	2) 막대 - 집단별 집계 결과, 합계, 평균, 빈도
	3) 원 - 집단별 비율, 빈도
	4) 산전도* - x에 따른 y축, 값의 위치
2. 문장 (단어 빈도수)
	- 워드 클라우드
3. 지도 시각화

7/4 데이터 시각화 2:
- 연습문제 2 - 2행1열 그래프 생성, 각각 twix 사용
- %f 는 소수점 6자리까지만 추출

7/13 Mecab:
- 명령 프홈프트 = $ cmd
- 파워쉘 프롬프트 =  $ powershell # 이라고 terminal에 씀
- mac은 add.user.sh <-에 추가 (mecab폴더 안에서 해야 됨))
- mac 에서 mecab이 설치 안 돼서 강사님이 며칠 뒤에 연구해본 후 알려주신다고 하셨음

7/26 주성분분석:
- 종속변수(통제요인)를 판별(넣을지 안 넣을지)할 때 전에 T-Test 검정을 통해 선별함

7/27 주성분분석:
- 명명규칙:
	- 원본 -> df -> 독립변수(x_train)/종속변수(y_train)
	-> 표준화적용 독립변수(x_train_std_df)/종속변수(y_train_std_df)
		- "train"은 '연습한다'라는 뜻
- pca 분석을 사용하여 주성분 분석 시 데이터 표준화 필수
- pca 분석을 안 쓴다면 표준화는 필수가 아니다

8/8 시계열분석:
- Google Colab: 
	- 파이썬 용량이 너무 클 때에는 여기다가 대신 실행 가능
	- 한글(글꼴도) 지원 안함 - linux 명령어를 써서 직접 설치 필요

8/10 선형회귀:
- 회귀형 분석은 보통 카테고리(범주형) 데이터는 숫자로 변환하여 종속변수와
연관성이 있는지 검증한 후 (샤피로, 등등) 한다.
- 회귀 분석은 어떤 독립 변수가(들이) 종속 변수에 가장 영향을 주는지 알아보는 것이 목적
- 샤피로 검정는 데이터(표본)의 수가 50 미만인 경우만 사용한다
- Kolmogorov Smirnov Test은 데이터(표본)의 수가 50 기상인 경우 사용
- PCA 분석은 무조건 표준화 해야 된다
- boxplot을 그릴 경우 data=데이터프레임 으로 주면 알아서 범주형 데이터는 빼고 돌려줌
	- boxplot을 하나씩(컬럼별) 할 경우
- 선형회귀 - 다이아몬드예시.ipynb 마지막에 잔차 분석도 해야 됨
(그런데 시간이 너무 오래 걸려서 수업에서는 안 함)
- ols (ordinary least squares) regression이란?
	- : a method that allows us to find a line that best describes 
	the relationship b/w one or more predictor variables and a 
	response variable.

8/11 금:
- ols 분석 하기 전까지는 분석이 아니라 데이터 전처리이다
- 시간이 오래 걸리는 시각화는 파일 저장해 놓으면 좋음:
	- plt.savefig('파일명.png', dpi= )

getter-setter 설치 방법:
- ctrl + shift 누르면 getter, setter 추가됨 (전에 extension 설치 해서 되어있을 것임)

8/29:
- scikit-learn은 utility의 기능을 습득하는 과정이다
	- from sklearn.impute import `SimpleImputer`
	- from sklearn.model_selection import `train_test_split`
    - from sklearn.preprocessing import `StandardScaler`,`PolynomialFeatures`
	- from sklearn.linear_model import `LinearRegression`
	- from sklearn.metrics import `confusion_matrix`, `roc_curve`, `roc_auc_score`, `accuracy_score`, `recall_score`, `precision_score`, `f1_score`, `r2_score`, `mean_absolute_error`, `mean_squared_error`
- 회귀분석 수행 시 1차를 사용할지 2차를 사용할지 판단하는 기준은 산점도 그래프를 그려봐야 된다 
	- 점들의 배치가 직선/곡선인지 판단 후 사용여부 결정
- 발표 용 파일 저장하기 (산점도 그래프 + 추세선)
	- helper.regplot(... save_path="manhattan_%s.png" % 파일이름)

8/30 - tensorflow 소개:
- 사람이 계산하는 통계학이라는 분야에서
	- 통계 라이브러리
		- 통계학 안에서의 특정 연산들을 수행하는 통계 라이브러리들이 있다
		- 용도/전문분야에 따른 각 라이브러리
	- 머신러닝
		- 일반적인 컴퓨팅 프로그래밍으로 만들기 어려운 부분들을 머신러닝이 커버를 해준다
		- 머신러닝은 통계 라이브러리보다 좀 더 넓은 범주를 커버해준다
		- 지도학습, 비지도학습, 강화학습 등등
		- sklearn은 지도와 비지도만 포함한다
		- 강화는 대부분 게임프로그래밍에서 사용한다
	- 그러나 통계라이브러리와 머신러닝의 용도와 기능은 같다.
		- 통계라이브러리와 머신러닝 둘 다 자동화된 기능들이다 (사람이 직접 다 하기 어려운 기능을 제공)
		- 예) ax + b = y 에서 x에 숫자값을 넣는 것은 사람이 할 수 있었으나 그림, 동영상, 자연어 등을 
		대입할 시 사람이 처리를 못하는 경우 발생
		- 통계라이브러리에서도 시간이 너무 오래 걸려 할 수 없음
		- 그러므로 머신러닝 중 지도학습에서 인공신경망이라는 것이 등장
- 다항 선형 회귀의 경우 
	- 독립변수들을 2차항을 만들고 그 변수들을 선형회귀에 넣는 것
	- tensorflow는 2차항을 못 만든다 
		-> scikitlearn을 통해 2차항을 만든 후 그 변수들을 Sequential()에 넣으면 된다
		-> 대신, 입력값의 개수가 늘어날 것 (input_shape)
		-> 입력값의 개수가 늘어나면 계층 수도 늘어날 것이다

9/1 머신러닝 - 단순선형회귀분석:
- 프로젝트/보고서 제출할 때, 각 단계를 통해서 무엇을 알 수 있었는지/얻어내었는지/왜 했는지 명시해야 함
- 등분산성!!!
	- 머신러닝에서는 등분산성을 만족하는 데이터들만 분석하는 것이 기본
	- 등분산성 미충족 시 표준화 / 정규화 하면 됨
- 머신러닝에서는 독립변수 = 훈련데이터라 하고, 종속변수 = 레이블이라 칭함
- tensorflow와 다른 머신러닝 라이브러리들의 가장 큰 차이점:
	- tensorflow는 학습모델을 시스템에 적용 가능
	- java, javascript, 그 외 다른 프로그래밍 언어에 접목 가능
	- django 등에 적용 가능
	- tensorflow에서는 이전에 학습했던 모델을 파일로 저장해서 그걸 로딩한 다음,
		거기에 데이터를 추가시켜서 이어서 학습 가능
9/4
- 다항회귀 표준화:
	- 표준화 후 다항식으로 변환하는 것이 분석/예측 결과가 훨씬 좋다!!
	- 꼭 표준화 후 독립변수들을 다항식으로 변환하여라!
	- 파일: F_데이터마이닝, 3_tensorflow, 연습문제1(다항회귀)
- 다항식 변환 중 이진 변수:
	- 독립변수들 중 명목형 변수 중 더미변수 중 이진법 (0과 1만 되어 있는 수)은 다항식으로 변환 안 해도 됨
	- 다른 독립변수들을 다항식으로 변환할 때 이 더미변수들은 제외한 후 다항식으로 변환한 후 분석 진행

9/6
- 앞으로 엑셀에서 파이썬 코드를 사용할 수 있을 것
- 머신러닝 학습 모델을 만든 다음, 제대로 된 건지 비교할 수 있는 방법:
    - 일반 통계 분석에서 statsmodels 로지스칙회귀 분석으로 실행해봐서 결과 비교
    - 차이가 심하게 나면 모델의 방향성을 잘못 잡은 것
    - 차이가 얼마 안 나면 제대로 간 것
- 변수 이름이 중간에 '.'가 있어서 인식을 못할 때 그 변수명을 역다음표(``)로 감싸주면 인식 가능
- 명목형 변수에서 표준화는 필요한 경우 값이 2개 이상인 애들만 해주면 됨
	- 0과 1로 되어있는 것들은 안 해줘도 됨
- y(독립변수)는 표준화 반드시 할 필요 없음 (0과 1로 되어있기 때문)
	- 상황에 따라서 해줘도 안 해줘도 됨, 필수는 아닌 것 같다 (다윤)
- 병원 분야는 높은 정확도를 필요로 한다 = 생사가 달린 문제이기에
	- 그래서 웬만하면 이쪽으로 가지 말아라 전문적인 지식이 없으면- 어렵다
- 다루는 데이터가 많아지면 변수이름을 한글로 사용하는 것을 적극 활용하기를 권함
- 중간중간에 데이터를 파일에 저장해놓기
	- 예) 엑셀 excel
		- df상권영역2.to_excel('df상권영역2.xlsx', index=False)
- 중복 제거하는 함수: 데이터프레임.drop_duplicates() -> 행 단위로 중복되는 데이터 제거
- 데이터 전처리 경우 상관계수를 확인할 수도 있지만 시간이 너무 오래 걸릴 것이기에
	- 어떤 논문에서는 전처리를 이렇게 하였다(예-요일별 매출은 제거하고 주중, 주말 매출만 포함)
	(예-건수 제외) 라고 참고하며 일일히 상관계수를 확인 안 하는 방법도 있다

9/7
- 명목형변수
	- 명목형뱐수의 기초통계량확인은 최솟값 최댓값을 확인하려는 것보다 종류가 몇개인지,
	분포가 한 쪽으로 취우쳐졌는지 아닌지 등등을 확인
	- 명목형변수의 이름만 추출:
		cnames = []
		for i, v in enumerate(df.dtypes):
			if v == 'category':
				cnames.append(df.columns[i])	
		cnames
- 순서 있는 범주와 순서 없는 범주
	- (F_데이터마이닝, 3_Tensorflow, 연습문제1_다풀(이항분류).ipynb) 참조
		data_pre2 = data_pre1.copy()
		data_pre2
		# 순서 없는 범주 설정
		ctype = ['sex', 'Recur', 'stage', 'smoking', 'obesity']
		for a in ctype:
			data_pre2[a] = data_pre2[a].astype('category')
		data_pre2.dtypes

		# 순서 있는 범주 설정
		n_ctype = ['post.CA19.9.binary', 'post.CA19.9.3grp']
		n_cat_dict = {}
		for a in n_ctype:
			df = DataFrame(data_pre2[a].value_counts().sort_index())
			n_cat_dict[a] = list(df.index)
			print('n_cat_dict[%s]: ' % a, n_cat_dict[a])

			cat_type = CategoricalDtype(categories=n_cat_dict[a], ordered = True)
			data_pre2[a] = data_pre2[a].astype(cat_type)
		data_pre2.dtypes
	
 9/8 소프트맥스 (다항분류)
 - .unique() 함수 - 고유한 값만 반환
	- .value_counts() 함수 해도 됨
- 종속변수 라벨링
    - `cat.rename_categories({'값1': 0, '값2': 1, '값3': 2, ...})`
    - `.np.where(condition, x, y)`함수 활용해도 됨
        - condition -> 조건
        - x -> condition이 true면 x반환
        - y -> condition이 false면 y반환
- numpy.argmax(값들, axis= -1) - 가장 큰 값의 인덱스 출력하는 함수
	- axis = -1 은 축의 방향
- MinMaxScaler로 표준화한 데이터들을 학습하는 것이 StandardScaler로 표준화한 
	데이터들을 학습하는 것보다 학습 속도가 더 빠르다

9/11
- pip 패키지 (보안문제로 회사 내에서 pip 설치가 불가능할 때 pip 설치 방법 - 추후 보안부서에 문의)
	1) 현재 설치된 목록을 텍스트 파일로 저장
		- pip3 freeze > 텍스트파일경로
		- 패키지 목록을 백업하는 것
	2) 텍스트파일에 명시된 패키지들을 일괄 설치 
		- pip3 install --upgrade -r 텍스트파일경로
		- 단, 이 방법은 네트워크가 가능한 환경에서만 동작
	3) 외부컴퓨터에서 pip를 통해 패키지들을 특정 폴더에 다운로드받기
		- pip3 download -d 저장될폴더경로 -r 텍스트파일경로
		- ex) pip download -d ./hello -r pypackage.txt
	- 다운로드 후 패키지 목록이 백업된 텍스트 파일을 패키지 폴더안에 함께 보관
		-> 이 폴더를 통째로 설치하고자하는 PC에 가져감
	- 설치시에는 패키지와 텍스트파일이 포함된 폴더 위치에서 명령프롬프트를 실행
		- pip install --no-index --find-links -r 텍스트파일경로
		- ex) pip install --no-index --find-links -r pypackage.txt
- 정확도, 손실률, 오즈비 등 값들이 컴퓨터마다 다르게 나오는 이유는 train_test_split할 때,
	컴퓨터의 CPU마다 다르게 훈련 데이터와 검증 데이터를 나누기 때문에 다르게 입력된 값들에 따라서
	학습 결과 값들이 미세하게 다를 것이다
- Google Colab
	- docker와 비슷
	- 하나의 리눅스와 같다
	- 파일 하나가 하나의 이미지로 저장됨
	- 한글 사용 시 한글 글꼴을 깔아줘야 됨 -> 리눅스 명령어로 한글 글꼴 다운로드
		- !sudo apt-get install -y fonts-nanum
		- !sudo fc-cache -fv
		- !rm ~/.cache/matplotlib -rf
		-> 이렇게 입력 후 실행하여 한글글꼴을 다운로드한 후
		- from matplotlib import font_manager as fm
		- import matplotlib.pyplot as plt
		- fe = fm.FrontEntry(
			fname = r'/usr/share/fonts/truetype/nanum/NanumGothic.ttf', # .ttf 파일이 저장되어 있는 경로
			name = 'NanumGothic') 									# 이 폰트에 원하는 이름 설정
		- fm.fontManager.ttflist.insert(0, fe)						# Matplotlib에 폰트 추가
		- plt.rcParams.update({'font.size': 18, 'font.family': 'NanumGothic'})	# 폰트 설정
	- 왼쪽 메뉴에 '폴더그림'을 선택하면 run time한다고 한 후 뜸
	- 현 디렉토리는 'content' 폴더에 위치
	- 해야할 것들:
		- google colab을 열면 가장 먼저 사본을 '저장'부터 하고 시작하기 (drive나 github에)
		- 그 후 저장한 파일을 열고 무조건 위측 메뉴에 '런타임' > '런타임 유형 변경' > 'T4 GPU' 선택
			- 런타임 유형을 변경하는 건 컴퓨터 종류를 선택하는 것
				- TPU: 구글에서 만든 tensorflow 전용 GPU
			- 런타임 유형을 바꿔버리면 처음서부터 실행을 다시 해야됨 (컴퓨터 바꾸는 것이랑 비슷)
			- -> 그래서 이것부터 해야하는 것
			- 비활성화 되어있는 A100 GPU와 V100 GPU는 유료옵션임
			- 'CPU' 보다 'T4 GPU'가 빠르고, 최소 내 컴퓨터보단 빠를 것이다
	- 웬만한 pip 패키지 사용 가능
		- `!` 후 `pip3`를 통해 pip 명령어 사용 가능
		- `! pip freeze`하면 사용 가능한 pip 패키지들 출력

9/12 tensorflow - CNN image
- 머신러닝은 코드의 결을 하나 만들어 놓고 거기에 들어가는 하이퍼 파라미터들을 계속 바꿔가면서
	최적의 결과가 나올 때까지 여러번 실험하는 것 

9/13
- 모델의 성능을 어떻게 향상시켰는가 - 인터뷰 때 포커스를 맞춰야 함

9/14 tensorflow 18_CNN_CIFAR-10
- visual studio에서 변수값들 확인: Variables(+Code 버튼 있는 상단 맨 오른쪽에 있음)
- CNN = 이미지 분류 전용
- RNN = 텍스트 분류 전용 (실은 요즘 잘 안씀)
- listdir(경로) -> 경로 안에 있는 폴더 목록을 추출한다

9/20
- QGIS에서 맥 한글 설정: euc-kr

9/22
- 프로젝트 - assignment3만 제출하고 시작하면 됨 (이번에는 assign.1/2는 생략)
- 행정동: 주민센터 단위로 나뉘어진 것
	- 행정동은 몇 개의 법정동을 묶는다

9/25 QGIS - 최단거리 측정
- 역세권 기준: 역에서 420m 내외
- QGIS reference website

9/26 QGIS - 보간법
- QGIS에서는 IDW 사용

10/4 최종 프로젝트
- 다음주 수요일까지 최종프로젝트 주제 선정 후 proposal 양식 그대로 작성 후 제출 (선행자료 같이 제출)
- 데이터
	- 상권분석 웹 서비스 (https://sg.sbiz.or.kr/godo/index.sg)
	- 디지털 시민 시장실 (http://scpm.seoul.go.kr/realTimeCity/ttareungiSubBigData)
		- 서울시에서 입지 잡아주는 사이트 (상권에 대해서, 대중교통, 공공자전거 시설물, 대여 반납 예측 결과 등)
		- 범용적으로 사용 불가능 - 특정한 변수 별 조회 가능
- proposal
	- 나랑 argument가 비슷한 기존 논문/컬럼 2-3개 (공신력 있는 기관에서 발행한 백서 등)을 메인으로 잡고 
	거기 있는 내용들을 참고하면서 내 주장에 근거로 삼아야 됨
	- 절대 기사 인용하지 않기 - 마이너스가 될 것임
	- 서론에서 문제를 제기할 때 기사를 사용하는 것은 괜찮으나 중간에서 우리들의 주장을 밑받침할 근거로 인용하면 안 된다
	- 보도 자료 인용은 괜찮다
	- 강사님 같은 경우는 다른 사람들은 어떻게, 무엇을 기준으로 입지 선정을 정하였는지, 
	어떤 분석 기법을 사용하였는지를 알아보았다 - 그들과 겹치지 않기 위해
	- 강사님께서 '다른 성행 연구들은 이러 이러한 방법을 써서 나와 같은 목표를 추구했다'라는 것을 적으신 것
	- 근거로 잡기 좋은 자료:
		- 한국 갤럽 조사 연구소*****
			- 서론에서 근거로 넣기 되게 좋음!!!!!
			- 포트폴리오 앞 부분에 우리들의 주제랑 비슷한 설문조사가 있으면 이 설문 내용을 근거로 넣기!! 
			우리들의 주장을 밑받침하는 데 도움이 될 것임! - 이런 설문 조사를 근거로 삼음!
		- 글로벌 리서치
- 1. 서론: 내가 무엇을 하고 왜 할 것인지, 내가 하고자 하는 것은 이런 것이다 라고 정의, 그 후 분석 프로세스 제시
	- 주제 소개: 포트폴리오 준비 시 <선행연구 분석-사전조사>이 중요하다 
		- why? 근거 자료 - 설문 결과, 기사(신문사, 네이버 제외)
	- 용어 설명: 포트폴리오에 용어 설명 필수! (다 알겠지 하고 넘어가면 안 됨)
		- 명확한 출처 필요
	- 사용 데이터 소개, 데이터 전처리 과정
		- 소스코드 너무 디테일하게 안 줘도 됨 - 너무 디테일하면 지루해진다
		- 전처리 과정은 대략적으로 넣으면 됨 (포르폴리오에 특히!)
			- 우리 수업에서는 강사님이 우리가 어떻게 무엇을 배운지 보여주기에 점수에 좋게 반영되지만 
			포트폴리오에는 많이 간략하게 해도 됨!
			- 어떤 데이터를 받아서 어떻게 처리를 했다 (before, after 정도)
			- 내가 어떤 기법을 활용했다 정도로 글로 적으면 됨, 소스코드 다 넣을 필요 없음
		- 추가 변수를 넣었을 때 왜 넣었는지 설명/언급!
			- 추가 변수의 필요성 설명할 때 출처를 넣으면 추가변수 필요성/중요성에 힘이 있어진다
			- 예) 「2020 서울시 빅캠공모전」_「어서와!」_분석결과서.pdf(8) 에서 '정보화수준점수'라는 
			파생변수를 만들 때 계산한 공식의 출처를 밝힘 (-> 서울시 열린 데이터 광장)
	- 변수 요약 (시각화 자료)
- 2. 본론
	- 사용한 분석 방법 설명 (2-3가지 이상): 이론 설명
		- 예) 「2020 서울시 빅캠공모전」_「어서와!」_분석결과서.pdf(10)에서 사용한 기법인 '클러스터링' 소개
		- 분석 과정 생략!
	- 결과 비교
	- 최종 결과 선정 과정 및 결과
- 3. 결론
	- 주장
		- 입지 선정 경우, 강사님 파워포인트 결론 (스타벅스입지선정(송파구중심).pdf(29) 참조)이랑 
		클러스터링 결과를 비교해서 제시하는 것이 가장 좋다
	- 의의(활용 방안)
		- 예를 들면, 우리가 만들어낸 결과를 듣는 회사에게, '당신들은 이 결과를 이렇게 활용할 수 있을 것입니다'
		라고 제시
	- 한계점
		- 예) 이러한 데이터가 있었더라면 분석 결과가 더 좋았을 수도 있다, 데이터의 접근성에 한계가 있었기 때문에
		이러이러한 것들을 적용하지 못하였다 등
- 참고문헌
- 피드백은 종강하고 2주일 후 보내줄 것임
- 재현성: 소스코드는 별도로 제출하기! (파워포인트에는 미포함)
- 가중치를 계산할 때 근거되는 자료를 포함해주면 더 좋다 (논문 등 참조)
- 발표:
	- 마지막 7번 보고서 폴더에 있는 내용 양식을 우리가 발표 파워포인트에 넣고 발표할 것
	- 요약은 생략
	- 연구 배경 및 목적: 내가 이것을 왜 하고 싶다 명시
	- 연구 범위: 시간적, 공간적 범위
	- 문헌검토: 선행연구조사 내용 설명 (입지의 개념 - 강사님꺼 써도 됨, 그러나 참고하신 출처 똑같이 밝히기
	, 상권을 입지 조건에 넣어야 하는 이유등을 쓴 것, 입지 선정의 중요성 - 내가 하려는 것이 왜 중요한가,
	스타벅스 입지 조건 등등)
	- 연구 방법: 
		- 데이터 구축 **: 신뢰할 수 있는 데이터임을 알려줘야 됨: 데이터 수집 방법/출처
			- 변수 설정: 선정한 변수에 관해 변수 표 작성
		- 분석 방법: QGIS에서 어떤 방법을 사용해서 분석을 했고, 어떠한 전처리 결과를 얻었다
	- 분석 방법:
		- 데이터 전처리 후 어떠한 방법으로 분석을 하였다 (글로 서술), 파워포인트에는 보고서에서보다 더 간결하게 서술
		- 파이썬 사용하는 경우 소스 코드 미포함!
		- 가중치 뽑았고, 결과가 어떻게 나왓다 등
	- 결과
		- 연구 결과: 최종적으로는, 스벅 위치들을 분석하고 QGIS을 사용하여 최적의 위치를 확인했다 - 선행연구에서는 어떠했는데
		나는 이를 바탕으로 추가적으로 스벅 매장간의 거르에 가중치를 적게 부여하여 최적의 입지를 확인함
		- 연구 한계: 예) 기존 스벅 평균 매출액이 잇었으면 좋았을 것 같은데 수집할 수 있는 자료가 아니어서 못 넣었다, MZ세대만이
		아닌 타겟을 더 넓게 잡으면 더 정밀한 결과를 산출할 수 있었을 것, 사이렌 오더 데이터, 배달 서비스 데이터 등등은 과제로 '너네가
		도입해서 해 봐라'라는 식으로 간접적으로 제시하는 것
	- 참고문헌
		- 출처표기법 준수!
			- CHICAGO 스타일 (https://essayreview.co.kr/chicago-citation-generator)
			- 한국 공학계열은 시카고 스타일 많이 사용

10/5
- ?cross_val_score 이라고 치면 cross_val_score의 파라미터 종류 확인 가능

10/6
- info() 함수 -> memory usage 확인
- 실제 데이터의 정확도는 50~60%만 나와도 괜찮다고 여겨진다 (의학이 아닌 이상)
- 취업:
	- 탐색적 데이터 분석, 대시보드 작성, 
	- SQL 잘 해야 됨
	- 예측이랑 분류 중 분류를 많이 씀
		- ab test 많이 씀 - 둘 중 하나 고르는 것
	- 도메인 지식 사전 조사 중요!!
- 패키지 참조 시
	- my.py 라는 파일 생성 후 그 안에 자주 쓰는 import구문 넣어놓기
	- from my import *
	- import문 유형은 여러가지가 있음!
- 서비스업에서는 이런 성능평가도 같이 제기해야 된다
	- 이유: 성능평가에 따라 하드웨어 견적이 바뀌기 때문
- random_state는 재현성을 위해서 넣는 것이지, 실무에서는 웬만하면 사용하지 않는 것을 추천
- Google Colab은 신경망에서는 유리하지만 다른 것들은 그저 그렇다

10/10 분류/군집 연습문제, 따릉이 군집, 따릉이 군집 QGIS (영상 확인)
- 분류: 종속변수가 2개가 넘어가면 정밀도, 재현율, f값은 잘 안 봄!
- knn과 kmeans는 차원의 수가 많으면 많을수록 시간이 많이 오래 걸린다
	- 종속변수가 4개 이상이 되면 knn과 kmeans를 사용하는 것보다 pca분석 하는 것을 고려할 것 (더 빨리 되기 때문에)
- dbscan은 차원의 수가 많아도 시간이 많이 오래 걸리지는 않는다

10/11 계층적군집
- 종속 변수가 이미 있는 데이터는 왜 군집 분석을 하는가?
	- 기존의 종속 변수가 적합한지를 판별하기 위해 
	- 기존의 종속 변수 재배치가 필요한지 아닌지 판단하기 위해
	- 예) 현재 있는 시설물들이 적합하게 배치되어 있는가, 적합하지 않다면 재배치를 위한 정책이 필요한가 등

10/13
- 취업:
	- 데이터베이스 기업 - 회귀분석, 분산분석 중요
	- 데이터베이스를 도입하는 기업 - 불특정 고객에 대한 트랜드를 읽어야 되기에 후반부 알고리즘 중요
- 현업:
	- 고객층을 분류할 때 전문가들과 회의로 진행
	- 실무에서는 변수 선정의 이유에 대힌 근거자료들을 제시해줘야 됨
앞으로 할 것들
- 알고리즘
	1) 의사결정츠리
	2) 서포트 백터 머신
	3) 앙상블 -> 베깅, 부스팅
	4) 프로헷(시계열)
- sklearn에서의 fit은 따로 객체에 안 담아도 됨, tensorflow에서는 fit으로 학습한 것을 따로 객체에 담아야됨

10/19
- predict() 함수는 데이터프레임을 추출한다

10/20
- 머신러닝에서 데이터 표준화 무조건 걸기!
- 머신러닝에서는 무조건 train test 용으로 데이터 분리하기!
- 머신러닝에서 무조건 학습은 train으로 하고 예측은 test로 하기 (정석)
- 머신러닝에서 무조건 성능평가 (score)도 test로 하기 (정석)

r값 (상관 분석)
- = 상관계수
- correlation coefficient
- 두 변수 x, y 사이의 상관관계의 정도를 나타내는 수치
- 범위: -1 ~ 1
- 상관계수의 절대값의 크기 = 직선관계에 가까운 정도를 나타냄
- 상관계수의 절대값의 부호 = 직선관계의 방향을 나타냄
- r > 0 (양의 상관관계): 산점도에서 점들이 우상향방향으로 띠를 형성; 변수1의 값이 작으면 변수2의 값도 작다
- r < 0 (음의 상관관계): 산점도에서 점들이 우하향방향으로 띠를 형성; 변수1의 값이 작으면 변수 2의 값은 크다
- r = +1: 모든 점이 정확히 기울기가 양수인 직선 위에 위치
- r = -1: 모든 점이 정확히 기울기가 음수인 직선 위에 위치
- 상관계수의 값이 1 또는 -1에 가까울수록 두 변수 사이의 연관성이 크고, 0에 가까울수록 연관성이 매우 약함을 의미

r^2값 (회귀 분석)
- = 결정계수 = 상관계수를 제곱한 값
- 범위: 0 ~ 1 
- 상관 분석이 아닌 회귀 분석에서 사용하는 수치
- 회귀모델의 성능에 대한 평가 지표로서 회귀모델에서 독립변수가 종속변수를 얼마나 잘 설명해주는지 보여주는 표
- 높을수록 잘 설명한다는 뜻 -> 1에 가까울수록 해당 선형 회귀 모델이 해당 데이터에 대한 높은 연관성을 갖고 있다고 해석
- 그러나 독립 변수의 수가 증가하면 r^2 값이 함께 증가한다 -> 결정계수에만 의존하여 회귀모델을 평가하기에는 무리

f1-score (분유 모델)
- 분류 모델에서 사용되는 머신러닝 평가지표
- 범위: 0 ~ 1
- 높을수록 모델이 좋다는 평가를 받는다
- f1 = 2 * (Precision정밀도 * Recall) / (Precision + Recall)

t-값
- t-값의 절대값이 클수록 p-값은 작아지며, 귀무 가설에 대한 증거가 커집니다
- 보통 t-검정은 모집단 평균(2-표본 t) 간 유의한 차이 또는 모집단 평균과 귀무 가설에서의 값(1-표본 t) 간 유의한 차이를 찾기 위해 실시
- t-값은 표본 데이터 변동에 비례한 차이의 크기를 측정
- 다르게 표현하면 T는 계산된 차이를 표준 오차 단위로 나타낸 것으로, 
- T의 크기가 클수록 귀무 가설에 대한 증거가 큼 -> 즉, 유의한 차이가 있다는 증거가 더 명확
- 반면 T가 0에 가까울수록 유의미한 차이가 없을 가능성이 커짐

VIF값
- VIF (Variance Inflation Factor)란, `분산 팽창 인수`라고 합니다
    - 이 값은 다중회귀분석에서 독립변수가 `다중 공산성(Multicollnearity)`의 `문제를 갖고 있는지 판단`하는 기준
	- 주로 `10보다 크면 그 독립변수는 다중공산성이 있다`고 말함

pip command *** 맥은 반드시 pip가 아닌 pip3로 해야 됨
- pip3 install --upgrade package_name
	: installs package_name to the latest version
- pip3 show package_name
	: shows information about package_name
- pip3 list
	: shows lists of existing packages
- python3 -m pip install --upgrade pip
	: pip upgrade

Python API
- https://facebook.github.io/prophet/docs/quick_start.html

노트북
- lg gram
- 외장하드 따로 붙일 수 있는 것
- GPU 큰 것

function parameter 확인
- import inspect
- inspect.signature(함수이름)