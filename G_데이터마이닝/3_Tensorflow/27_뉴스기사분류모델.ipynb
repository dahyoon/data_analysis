{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 뉴스기사 분류모델\n",
    "## 1. 형태소 분석엔진 설치 (Colab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/SOMJANG/Mecab-ko-for-Google-Colab.git\n",
    "!bash Mecab-ko-for-Google-Colab/install_mecab-ko_on_colab_light_220429.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 패키지 참조\n",
    "### 1) helper 참조"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "# sys.path.append('../../')\n",
    "import helper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) 다른 패키지들 참조"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import seaborn as sb\n",
    "import requests\n",
    "\n",
    "from pandas import DataFrame, read_excel\n",
    "from konlpy.tag import Mecab\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embeddint, Dense, GRU\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequences import pad_sequences\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 데이터셋 준비하기\n",
    "### 1) 뉴스기사 데이터셋"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "origin = read_excel('./news.xlsx')\n",
    "origin.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) 불어용 데이터셋"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = requests.get('https://data.hossam/kr/korean_stopwords.txt')\n",
    "r.encoding='utf-8'\n",
    "stopwords = r.text.split('\\n')\n",
    "print(stopwords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 데이터 전처리\n",
    "### 1) 뉴스기사에서 영어, 특수문자를 제거하고 한글만 남기기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = origin.copy()\n",
    "\n",
    "# 한글을 제외한 나머지 글자들을 빈 문자열로 대체\n",
    "df['content'] = df['content'].str.replace('[^ㄱ-ㅎㅏ-ㅣ가-힣 ]', '', regex=True)\n",
    "\n",
    "# document 컬럼의 데이터들 중에서 빈 문자열만 존재하는 항목은 결측치로 대체\n",
    "df['content'].replace('', np.nan, inplace=True)\n",
    "\n",
    "# 전체 데이터 셋 크기 확인\n",
    "print('데이터 크기: ', df['content'].shape)\n",
    "\n",
    "# 결측치 확인\n",
    "print('결측치 크기: ', df['content'].isna().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) 결측치 제거"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/dayoonz/Desktop/data_analysis/G_데이터마이닝/3_Tensorflow/27_뉴스기사분류모델.ipynb Cell 14\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/dayoonz/Desktop/data_analysis/G_%EB%8D%B0%EC%9D%B4%ED%84%B0%EB%A7%88%EC%9D%B4%EB%8B%9D/3_Tensorflow/27_%EB%89%B4%EC%8A%A4%EA%B8%B0%EC%82%AC%EB%B6%84%EB%A5%98%EB%AA%A8%EB%8D%B8.ipynb#X21sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m df\u001b[39m.\u001b[39mdropna(inplace\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/dayoonz/Desktop/data_analysis/G_%EB%8D%B0%EC%9D%B4%ED%84%B0%EB%A7%88%EC%9D%B4%EB%8B%9D/3_Tensorflow/27_%EB%89%B4%EC%8A%A4%EA%B8%B0%EC%82%AC%EB%B6%84%EB%A5%98%EB%AA%A8%EB%8D%B8.ipynb#X21sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m df\u001b[39m.\u001b[39misna()\u001b[39m.\u001b[39msum()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "df.dropna(inplace=True)\n",
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3) 종속 변수 라벨링\n",
    "#### 종속변수 값 확인 및 종류를 딕셔너리로 변환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "EOL while scanning string literal (4234776867.py, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[2], line 2\u001b[0;36m\u001b[0m\n\u001b[0;31m    category = list(df['category].unique())\u001b[0m\n\u001b[0m                                           ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m EOL while scanning string literal\n"
     ]
    }
   ],
   "source": [
    "# 종속 변수 값의 종류\n",
    "category = list(df['category].unique())\n",
    "category\n",
    "\n",
    "# 종속 변수 값의 종류를 딕셔너리로 변환\n",
    "cat_dict = {}\n",
    "for i, v in enumerate(category):\n",
    "    cat_dict[v] = i\n",
    "cat_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 종속 변수 라벨링"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['category'] = df['category'].map(cat_dict)\n",
    "df['category'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4) 뉴스 기사 형태소 분석"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if sys.platform == 'win32':\n",
    "    mecab = Mecab(dicpath=\"C:\\\\mecab\\\\mecab-ko-dic\")\n",
    "else:\n",
    "    mecab = Mecab()\n",
    "\n",
    "# 문장내 형태소들을 저장할 리스트\n",
    "word_set = []\n",
    "\n",
    "# 덧글 내용에 대해 반복 처리\n",
    "for i, v in enumerate(df['content']):\n",
    "    # 덧글 하나에 대한 형태소 분석\n",
    "    morphs = mecab.morphs(v)\n",
    "    # print(morphs)\n",
    "    # if i > 5:\n",
    "    #     break\n",
    "\n",
    "    # 형태소 분석 결과에서 불용어를 제외한 단어만 별도의 리스트로 생성\n",
    "    confirm_words = []\n",
    "    for j in morphs:\n",
    "        if j not in stopwords:\n",
    "            confirm_words.append(j)\n",
    "\n",
    "    # 불용어를 제외한 형태소 리스트를 통째로 word_set에 저장함\n",
    "    # -> word_set은 2차원 리스트가 된다. 1차원이 덧글 단위임\n",
    "    word_set.append(confirm_words)\n",
    "\n",
    "# 상위 3건만 출력해서 확인\n",
    "word_set[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5) 형태소 토큰화\n",
    "#### 전체 단어에 대한 토큰화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(word_set)\n",
    "print(f'전체 단어수: {len(tokenizer.word_index)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3회 이상 자주 등장하는 단어의 수 구하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tokenizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/dayoonz/Desktop/data_analysis/G_데이터마이닝/3_Tensorflow/27_뉴스기사분류모델.ipynb Cell 24\u001b[0m line \u001b[0;36m5\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/dayoonz/Desktop/data_analysis/G_%EB%8D%B0%EC%9D%B4%ED%84%B0%EB%A7%88%EC%9D%B4%EB%8B%9D/3_Tensorflow/27_%EB%89%B4%EC%8A%A4%EA%B8%B0%EC%82%AC%EB%B6%84%EB%A5%98%EB%AA%A8%EB%8D%B8.ipynb#X41sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m threshold \u001b[39m=\u001b[39m \u001b[39m3\u001b[39m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/dayoonz/Desktop/data_analysis/G_%EB%8D%B0%EC%9D%B4%ED%84%B0%EB%A7%88%EC%9D%B4%EB%8B%9D/3_Tensorflow/27_%EB%89%B4%EC%8A%A4%EA%B8%B0%EC%82%AC%EB%B6%84%EB%A5%98%EB%AA%A8%EB%8D%B8.ipynb#X41sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39m# 전체 단어의 수\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/dayoonz/Desktop/data_analysis/G_%EB%8D%B0%EC%9D%B4%ED%84%B0%EB%A7%88%EC%9D%B4%EB%8B%9D/3_Tensorflow/27_%EB%89%B4%EC%8A%A4%EA%B8%B0%EC%82%AC%EB%B6%84%EB%A5%98%EB%AA%A8%EB%8D%B8.ipynb#X41sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m total_cnt \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(tokenizer\u001b[39m.\u001b[39mword_index)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/dayoonz/Desktop/data_analysis/G_%EB%8D%B0%EC%9D%B4%ED%84%B0%EB%A7%88%EC%9D%B4%EB%8B%9D/3_Tensorflow/27_%EB%89%B4%EC%8A%A4%EA%B8%B0%EC%82%AC%EB%B6%84%EB%A5%98%EB%AA%A8%EB%8D%B8.ipynb#X41sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m \u001b[39m# 등장 빈도수가 threshold보다 작은 단어의 개수를 카운트할 값\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/dayoonz/Desktop/data_analysis/G_%EB%8D%B0%EC%9D%B4%ED%84%B0%EB%A7%88%EC%9D%B4%EB%8B%9D/3_Tensorflow/27_%EB%89%B4%EC%8A%A4%EA%B8%B0%EC%82%AC%EB%B6%84%EB%A5%98%EB%AA%A8%EB%8D%B8.ipynb#X41sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m rare_cnt \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tokenizer' is not defined"
     ]
    }
   ],
   "source": [
    "# 사용 빈도가 높다고 판단할 등장 회수\n",
    "threshold = 3\n",
    "\n",
    "# 전체 단어의 수\n",
    "total_cnt = len(tokenizer.word_index)\n",
    "\n",
    "# 등장 빈도수가 threshold보다 작은 단어의 개수를 카운트할 값\n",
    "rare_cnt = 0\n",
    "\n",
    "# 훈련 데이터의 전체 단어 빈도수 총 합\n",
    "total_freq = 0\n",
    "\n",
    "# 등장 빈도수가 threshold보다 작은 단어의 등장 빈도수의 총 합\n",
    "rare_freq = 0\n",
    "\n",
    "# 단어와 빈도수의 쌍(pair)을 key와 value로 받는다.\n",
    "for key, value in tokenizer.word_counts.items():\n",
    "    total_freq = total_freq + value\n",
    "\n",
    "    # 단어의 등장 빈도수가 threshold보다 작으면\n",
    "    if(value < threshold):\n",
    "        rare_cnt = rare_cnt + 1\n",
    "        rare_freq = rare_freq + value\n",
    "\n",
    "print('단어 집합(vocabulary)의 크기 :',total_cnt)\n",
    "print('등장 빈도가 %s번 미만인 희귀 단어의 수: %s' % (threshold, rare_cnt))\n",
    "print(\"단어 집합에서 희귀 단어의 비율:\", (rare_cnt / total_cnt)*100)\n",
    "print(\"전체 등장 빈도에서 희귀 단어 등장 빈도 비율:\", (rare_freq / total_freq)*100)\n",
    "\n",
    "# 자주 등장하는 단어 집합의 크기 구하기 -> 이 값이 첫 번째 학습층의 input 수가 된다.\n",
    "vocab_size = total_cnt - rare_cnt + 2\n",
    "print('단어 집합의 크기 :', vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
